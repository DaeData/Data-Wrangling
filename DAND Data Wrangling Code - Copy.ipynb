{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapparser.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Your task is to use the iterative parsing to process the map file and\n",
    "find out not only what tags are there, but also how many, to get the\n",
    "feeling on how much of which data you can expect to have in the map.\n",
    "Fill out the count_tags function. It should return a dictionary with the \n",
    "tag name as the key and number of times this tag can be encountered in \n",
    "the map as value.\n",
    "\n",
    "Note that your code will be tested with a different data file than the 'example.osm'\n",
    "\"\"\"\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    for event, elem in ET.iterparse(filename, events=(\"start\",)):\n",
    "        if elem.tag in tags.keys():\n",
    "            tags[elem.tag] +=1\n",
    "        else:\n",
    "            tags[elem.tag] = 1\n",
    "    return tags\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "\n",
    "    tags = count_tags('dallas.osm')\n",
    "    pprint.pprint(tags)\n",
    "       \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\"\"\"\n",
    "Your task is to explore the data a bit more.\n",
    "Before you process the data and add it into your database, you should check the\n",
    "\"k\" value for each \"<tag>\" and see if there are any potential problems.\n",
    "\n",
    "We have provided you with 3 regular expressions to check for certain patterns\n",
    "in the tags. As we saw in the quiz earlier, we would like to change the data\n",
    "model and expand the \"addr:street\" type of keys to a dictionary like this:\n",
    "{\"address\": {\"street\": \"Some value\"}}\n",
    "So, we have to see if we have such tags, and if we have any tags with\n",
    "problematic characters.\n",
    "\n",
    "Please complete the function 'key_type', such that we have a count of each of\n",
    "four tag categories in a dictionary:\n",
    "  \"lower\", for tags that contain only lowercase letters and are valid,\n",
    "  \"lower_colon\", for otherwise valid tags with a colon in their names,\n",
    "  \"problemchars\", for tags with problematic characters, and\n",
    "  \"other\", for other tags that do not fall into the other three categories.\n",
    "See the 'process_map' and 'test' functions for examples of the expected format.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "    \n",
    "        key = element.attrib['k']\n",
    "        if re.search(lower, key):\n",
    "            keys['lower'] +=1\n",
    "        elif re.search(lower_colon, key):\n",
    "            keys['lower_colon'] +=1\n",
    "        elif re.search(problemchars, key):\n",
    "            keys['problemchars']+=1\n",
    "        else:\n",
    "            keys['other']+=1\n",
    "        pass\n",
    "    element.clear()\n",
    "    return keys    \n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    # You can use another testfile 'map.osm' to look at your solution\n",
    "    # Note that the assertion below will be incorrect then.\n",
    "    # Note as well that the test function here is only used in the Test Run;\n",
    "    # when you submit, your code will be checked against a different dataset.\n",
    "    keys = process_map('dallas.osm')\n",
    "    pprint.pprint(keys)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "\n",
    "def get_user(element):\n",
    "    return \n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if 'uid' in element.attrib:\n",
    "            users.add(element.get('uid'))\n",
    "   \n",
    "    print(len(users))\n",
    "    return users\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_map('dallas.osm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User_count.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "\n",
    "\n",
    "def get_user(element):\n",
    "    return \n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    users = set()\n",
    "    dup_att = {}\n",
    "   \n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if 'uid' in element.attrib:\n",
    "            dup_attrib = element.get('uid')\n",
    "            if dup_attrib in users:\n",
    "                dup_att[dup_attrib] +=1\n",
    "            else:\n",
    "                users.add(element.get('uid'))\n",
    "                dup_att[dup_attrib] = 1\n",
    "        element.clear()\n",
    "    for k,v in dup_att.items():\n",
    "        print (k, v)\n",
    "       \n",
    "                \n",
    "   \n",
    "\n",
    "    return users\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_map('dallas.osm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Apr  5 00:44:55 2018\n",
    "\n",
    "@author: Kris\n",
    "\"\"\"\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "#OSMFILE = \"dallas.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "#https://en.wikipedia.org/wiki/Street_suffix\n",
    "#https://pe.usps.com/text/pub28/28apc_002.htm\n",
    "#https://en.wikipedia.org/wiki/Texas_State_Highway_Spur_408\n",
    "expected = ['Alley', 'Annex', 'Arcade', 'Avenue', 'Bayou', 'Beach', 'Bend', 'Bluff', 'Bottom', 'Boulevard', 'Branch', 'Bridge',\\\n",
    "            'Brook', 'Burg', 'Bypass', 'Camp', 'Canyon', 'Cape', 'Causeway', 'Center', 'Circle', 'Cliff', 'Club', 'Common',\\\n",
    "            'Corner', 'Course', 'Court', 'Cove', 'Creek', 'Crescent', 'Crest', 'Crossing', 'Crossroad', 'Curve', 'Dale', \\\n",
    "            'Dam', 'Divide', 'Drive', 'Estate', 'Expressway', 'Extension', 'Fall', 'Ferry', 'Field', 'Flat', 'Ford', \\\n",
    "            'Forest', 'Forge', 'Fork', 'Fort', 'Freeway', 'Garden', 'Gateway', 'Glen', 'Green', 'Grove', 'Harbor', \\\n",
    "            'Haven', 'Heights', 'Highway', 'Hill', 'Hollow', 'Inlet', 'Island', 'Isle', 'Junction', 'Key', 'Knoll', \\\n",
    "            'Lake', 'Land', 'Landing', 'Lane', 'Light', 'Loaf', 'Lock', 'Lodge', 'Loop', 'Mall', 'Manor', 'Meadow', \\\n",
    "            'Mews', 'Mill', 'Mission', 'Motorway', 'Mount', 'Mountain', 'Neck', 'Orchard', 'Oval', 'Overpass', 'Park', \\\n",
    "            'Parkway', 'Pass', 'Passage', 'Path', 'Pike', 'Pine', 'Place', 'Plain', 'Plaza', 'Point', 'Port', 'Prairie', \\\n",
    "            'Radial', 'Ramp', 'Ranch', 'Rapid', 'Rest', 'Ridge', 'River', 'Road', 'Route', 'Row', 'Rue', 'Run', 'Shoal', \\\n",
    "            'Shore', 'Skyway', 'Spring', 'Spur', 'Square', 'Station', 'Stravenue', 'Stream', 'Street', 'Summit', 'Terrace', \\\n",
    "            'Throughway', 'Trace', 'Track', 'Trafficway', 'Trail', 'Trailer', 'Tunnel', 'Turnpike', 'Underpass', 'Union', 'Valley', \\\n",
    "            'Viaduct', 'View', 'Village', 'Ville', 'Vista', 'Walk', 'Wall', 'Way', 'Well', 'Wells', 'Via', 'Spur 408'] \n",
    "\n",
    "\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Av\": \"Avenue\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"road\": \"Road\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"blvd\": \"Boulevard\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"Dr.\": \"Drive\",\n",
    "            \"Expy\": \"Expressway\",\n",
    "            \"LN\": \"Lane\",\n",
    "            \"Ln\": \"Lane\",\n",
    "            \"Pkwy\": \"Parkway\",\n",
    "            \"Webb Chapel Rd 200\": \"Webb Chapel Road\",\n",
    "            \"I-30\": \"Interstate 30\",\n",
    "            \"I 30\": \"Interstate 30\",\n",
    "            \"Blvd E\": \"Boulevard E\",\n",
    "            \"West Main Street #B\": \"West Main Street\",\n",
    "            \"7815 McCallum Blvd 14203 Dallas TX 75252 Kjo\": \"McCallum Boulevard\",\n",
    "            \"I-20\": \"Interstate 20\",\n",
    "            \"I 20\": \"Interstate 20\",\n",
    "            \"Forest Central Drive, Suite 300\": \"Forest Central Drive\",\n",
    "            \"W Illinois Ave #306\": \"West Illinois Avenue\",\n",
    "            \"North Market Street #102\": \"North Market Street\",\n",
    "            \"75062\": \"\",\n",
    "            \"North Saint Paul Street, Suite 2010\": \"North Saint Paul Street\",\n",
    "            \"Canton Street, Suite 202\": \"Canton Street\",\n",
    "            \"Las Colinas Blvd E\": \"Las Colinas Boulevard East\",\n",
    "            \"Reunion Blvd E\": \"Reunion Boulevard East\",\n",
    "            \"North Pearl Street, Suite 1150\": \"North Pearl Street\"\n",
    "\n",
    "            }\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    streets = audit('dallas.osm')\n",
    "    pprint.pprint(streets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update_street_name.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sun Apr 22 17:11:19 2018\n",
    "\n",
    "@author: Kris\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "#https://en.wikipedia.org/wiki/Texas_State_Highway_Spur_408\n",
    "#https://en.wikipedia.org/wiki/Street_suffix\n",
    "#https://pe.usps.com/text/pub28/28apc_002.htm\n",
    "expected = ['Alley', 'Annex', 'Arcade', 'Avenue', 'Bayou', 'Beach', 'Bend', 'Bluff', 'Bottom', 'Boulevard', 'Branch', 'Bridge',\\\n",
    "            'Brook', 'Burg', 'Bypass', 'Camp', 'Canyon', 'Cape', 'Causeway', 'Center', 'Circle', 'Cliff', 'Club', 'Common',\\\n",
    "            'Corner', 'Course', 'Court', 'Cove', 'Creek', 'Crescent', 'Crest', 'Crossing', 'Crossroad', 'Curve', 'Dale', \\\n",
    "            'Dam', 'Divide', 'Drive', 'Estate', 'Expressway', 'Extension', 'Fall', 'Ferry', 'Field', 'Flat', 'Ford', \\\n",
    "            'Forest', 'Forge', 'Fork', 'Fort', 'Freeway', 'Garden', 'Gateway', 'Glen', 'Green', 'Grove', 'Harbor', \\\n",
    "            'Haven', 'Heights', 'Highway', 'Hill', 'Hollow', 'Inlet', 'Island', 'Isle', 'Junction', 'Key', 'Knoll', \\\n",
    "            'Lake', 'Land', 'Landing', 'Lane', 'Light', 'Loaf', 'Lock', 'Lodge', 'Loop', 'Mall', 'Manor', 'Meadow', \\\n",
    "            'Mews', 'Mill', 'Mission', 'Motorway', 'Mount', 'Mountain', 'Neck', 'Orchard', 'Oval', 'Overpass', 'Park', \\\n",
    "            'Parkway', 'Pass', 'Passage', 'Path', 'Pike', 'Pine', 'Place', 'Plain', 'Plaza', 'Point', 'Port', 'Prairie', \\\n",
    "            'Radial', 'Ramp', 'Ranch', 'Rapid', 'Rest', 'Ridge', 'River', 'Road', 'Route', 'Row', 'Rue', 'Run', 'Shoal', \\\n",
    "            'Shore', 'Skyway', 'Spring', 'Spur', 'Square', 'Station', 'Stravenue', 'Stream', 'Street', 'Summit', 'Terrace', \\\n",
    "            'Throughway', 'Trace', 'Track', 'Trafficway', 'Trail', 'Trailer', 'Tunnel', 'Turnpike', 'Underpass', 'Union', 'Valley', \\\n",
    "            'Viaduct', 'View', 'Village', 'Ville', 'Vista', 'Walk', 'Wall', 'Way', 'Well', 'Wells', 'Via', 'Spur 408', 'Central', \\\n",
    "            'Downs'] \n",
    "\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Av\": \"Avenue\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"road\": \"Road\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"blvd\": \"Boulevard\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"Dr.\": \"Drive\",\n",
    "            \"Expy\": \"Expressway\",\n",
    "            \"LN\": \"Lane\",\n",
    "            \"Ln\": \"Lane\",\n",
    "            \"Pkwy\": \"Parkway\",\n",
    "            \"Webb Chapel Rd 200\": \"Webb Chapel Road\",\n",
    "            \"I-30\": \"Interstate 30\",\n",
    "            \"I 30\": \"Interstate 30\",\n",
    "            \"Blvd E\": \"Boulevard E\",\n",
    "            \"West Main Street #B\": \"West Main Street\",\n",
    "            \"7815 McCallum Blvd 14203 Dallas TX 75252 Kjo\": \"McCallum Boulevard\",\n",
    "            \"I-20\": \"Interstate 20\",\n",
    "            \"Forest Central Drive, Suite 300\": \"Forest Central Drive\",\n",
    "            \"W Illinois Ave #306\": \"West Illinois Avenue\",\n",
    "            \"North Market Street #102\": \"North Market Street\",\n",
    "            \"75062\": \"\",\n",
    "            \"North Saint Paul Street, Suite 2010\": \"North Saint Paul Street\",\n",
    "            \"Canton Street, Suite 202\": \"Canton Street\",\n",
    "            \"Las Colinas Blvd E\": \"Las Colinas Boulevard East\",\n",
    "            \"Reunion Blvd E\": \"Reunion Boulevard East\",\n",
    "            \"North Pearl Street, Suite 1150\": \"North Pearl Street\",\n",
    "            \"N Central Expressway Ste 635\": \"N Central Expressway\",\n",
    "            \"South Parkway Boulevard South\": \"South Parkway Boulevard\",\n",
    "            \"E State Highway 356\": \"East State Highway 356\",\n",
    "            \"56th\": \"56th Street\",\n",
    "            \"Nile\": \"Nile Drive\",\n",
    "            \"Haskell\": \"Haskell Drive\",\n",
    "            \"Birchbrook\": \"Birchbrook Drive\",\n",
    "            \"North San Saba\": \"North San Saba Drive\",\n",
    "            \"South San Saba\": \"South San Saba Drive\",\n",
    "            \"Cardiff\": \"Cardiff Street\",\n",
    "            \"Barcelona\": \"Barcelona Drive\",\n",
    "            \"Jo Pierce\": \"Jo Pierce Street\",\n",
    "            \"Glenwood\": \"Glenwood Avenue\",\n",
    "            \"Gillette\": \"Gillette Street\",\n",
    "            \"Pleasant Mound\": \"South Buckner Boulevard\",\n",
    "            \"Sheree\": \"Sheree Lane\",\n",
    "            \"Wren\": \"Wren Way\",\n",
    "            \"Wandt\": \"Wandt Drive\",\n",
    "            \"Inwood\": \"Inwood Road\",\n",
    "            \"Vista Gate\": \"Vista Gate Drive\",\n",
    "            \"E Kearney\": \"East Kearney Street\"\n",
    "          \n",
    "            }\n",
    "\n",
    "def update_name(name):\n",
    "    m = street_type_re.search(name)\n",
    "    street_type = m.group()\n",
    "\n",
    "     if (street_type not in expected) & (street_type in mapping):\n",
    "         #https://lzone.de/examples/Python%20re.sub\n",
    "         name = re.sub(street_type, mapping[street_type], name)\n",
    "         \n",
    "\n",
    "    return name\n",
    "            print name, \"=>\", better_name\n",
    "            if name == \"West Lexington St.\":\n",
    "                assert better_name == \"West Lexington Street\"\n",
    "            if name == \"Baldwin Rd.\":\n",
    "                assert better_name == \"Baldwin Road\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### key_type.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Apr 16 18:43:42 2018\n",
    "\n",
    "@author: Kris\n",
    "\"\"\"\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "\"\"\"\n",
    "Your task is to explore the data a bit more.\n",
    "The first task is a fun one - find out how many unique users\n",
    "have contributed to the map in this particular area!\n",
    "\n",
    "The function process_map should return a set of unique user IDs (\"uid\")\n",
    "\"\"\"\n",
    "\n",
    "def get_keys(element):\n",
    "    return \n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if 'k' in element.attrib:\n",
    "            keys.add(element.get('k'))\n",
    "   \n",
    "\n",
    "    return keys\n",
    "\n",
    "def test():\n",
    "    keys = process_map(\"dallas.osm\")\n",
    "    pprint.pprint(keys)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import cerberus\n",
    "import schema\n",
    "\n",
    "from update_street_name import update_name\n",
    "\n",
    "\n",
    "OSM_PATH = \"dallas.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "    nodepos = 0 \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "   \n",
    "    \n",
    "    if element.tag == 'node':\n",
    "        for field in NODE_FIELDS:\n",
    "            node_attribs[field] = element.attrib[field]\n",
    "        for tag in element.iter('tag'):\n",
    "            t_dic = {}\n",
    "            t_dic['id'] = element.attrib['id'] \n",
    "            \n",
    "            \n",
    "            if PROBLEMCHARS.match(tag.attrib[\"k\"]):\n",
    "                continue\n",
    "                       \n",
    "            #split at colon into two separate values, like addr:street. Street becomes the key, addr becomes the type.\n",
    "            #If there is no colon set type to regular.\n",
    "            elif LOWER_COLON.match(tag.attrib['k']):\n",
    "                \n",
    "                t_dic['type'] = tag.attrib['k'].split(':')[0]\n",
    "                t_dic['key'] = tag.attrib[\"k\"].split(':',1)[1]\n",
    "                #correct all tags that associated with street.\n",
    "             \n",
    "                if tag.attrib['k'] == 'addr:street':\n",
    "             \n",
    "                    t_dic['value'] = update_name(tag.attrib['v'])\n",
    " \n",
    "                else:\n",
    "                    t_dic['value'] = tag.attrib['v']\n",
    "                \n",
    "            else:\n",
    "                t_dic['type'] = 'regular'\n",
    "                t_dic['key'] = tag.attrib['k']\n",
    "                t_dic['value'] = tag.attrib['v']\n",
    "            tags.append(t_dic)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "        \n",
    "    elif element.tag == 'way':\n",
    "        for field in WAY_FIELDS:\n",
    "            way_attribs[field] = element.attrib[field]\n",
    "#        Here we have to process the information slightly differently.\n",
    "#        Each id can have multiple entries so we keep them together and write them according to the id and position.\n",
    "#        We start at position 0, resetting the position when we encounter a new id.\n",
    "       \n",
    "        for nd in element.iter('nd'):\n",
    "            n_dic = {}\n",
    "            n_dic['id'] = element.attrib['id']\n",
    "            n_dic['node_id'] = nd.attrib['ref']\n",
    "            n_dic['position'] = nodepos\n",
    "            nodepos += 1\n",
    "            way_nodes.append(n_dic)\n",
    "        #repeat the same process for way as we did with node.\n",
    "        for tag in element.iter('tag'):\n",
    "            t_dic = {}\n",
    "            t_dic['id'] = element.attrib['id'] \n",
    "            if PROBLEMCHARS.match(tag.attrib[\"k\"]):\n",
    "                continue\n",
    "            elif LOWER_COLON.match(tag.attrib['k']):\n",
    "                t_dic['type'] = tag.attrib['k'].split(':')[0]\n",
    "                t_dic['key'] = tag.attrib[\"k\"].split(':',1)[1]\n",
    "                if tag.attrib['k'] == 'addr:street':\n",
    "                        t_dic['value'] = update_name(tag.attrib['v'])\n",
    "\n",
    "                else:\n",
    "                    t_dic['value'] = tag.attrib['v']\n",
    "\n",
    "            else:\n",
    "                t_dic['type'] = 'regular'\n",
    "                t_dic['key'] = tag.attrib['k']\n",
    "                t_dic['value'] = tag.attrib['v']\n",
    "            tags.append(t_dic)\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, codecs.open(WAYS_PATH, 'w') as ways_file, codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            \n",
    "            if el:\n",
    "\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove_blank.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['nodes.csv','ways.csv', 'ways_tags.csv', 'nodes_tags.csv', 'ways_nodes.csv']\n",
    "    \n",
    "for filename in files:\n",
    "    with open('{}_{}'.format('new',filename), 'w') as f_out:\n",
    "        for line in open(filename):\n",
    "            line = line.rstrip()\n",
    "            if line != '':\n",
    "                line = line + '\\n'\n",
    "                f_out.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### db_create.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Apr 24 17:11:15 2018\n",
    "\n",
    "@author: elikr\n",
    "\"\"\"\n",
    "# this script creates the sqlite3 database, creates the tables in the db and imports the information from each of the csvs created in the xml to csv conversion process in data.py.\n",
    "import csv, sqlite3\n",
    "sql_file=\"dallas_metro.db\"\n",
    "  \n",
    "con = sqlite3.connect(sql_file)\n",
    "cur = con.cursor()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS nodes; ''')\n",
    "con.commit()\n",
    "\n",
    "cur.execute(\"CREATE TABLE nodes (id INTEGER PRIMARY KEY NOT NULL, lat REAL, lon REAL, user TEXT, uid INTEGER, version TEXT, changeset INTEGER, timestamp DATE);\") # use your column names here\n",
    "con.commit()\n",
    "\n",
    "with open('nodes.csv','rb') as f: \n",
    "   \n",
    "    data = csv.DictReader(f) \n",
    "    to_db = [(i['id'].decode(\"utf-8\"),i['lat'].decode(\"utf-8\"),i['lon'].decode(\"utf-8\"),i['user'].decode(\"utf-8\"),i['uid'].decode(\"utf-8\"),i['version'].decode(\"utf-8\"),i['changeset'].decode(\"utf-8\"),i['timestamp'].decode(\"utf-8\")) for i in data]\n",
    "   \n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes (id, lat, lon, user, uid, version, changeset, timestamp) VALUES (?,?,?,?,?,?,?,?);\", to_db,)\n",
    "con.commit()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS ways; ''')\n",
    "con.commit()\n",
    "\n",
    "cur.execute(\"CREATE TABLE ways (id INTEGER PRIMARY KEY NOT NULL, user TEXT, uid INTEGER, version TEXT, changeset INTEGER, timestamp DATE);\") # use your column names here\n",
    "con.commit()\n",
    "\n",
    "with open('ways.csv','rb') as f:\n",
    "    \n",
    "    data = csv.DictReader(f) \n",
    "    to_db = [(i['id'].decode(\"utf-8\"),i['user'].decode(\"utf-8\"),i['uid'].decode(\"utf-8\"),i['version'].decode(\"utf-8\"),i['changeset'].decode(\"utf-8\"),i['timestamp'].decode(\"utf-8\")) for i in data]\n",
    "    \n",
    "\n",
    "cur.executemany(\"INSERT INTO ways (id,user,uid,version,changeset,timestamp) VALUES (?,?,?,?,?,?);\", to_db,)\n",
    "con.commit()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS nodes_tags; ''')\n",
    "con.commit()\n",
    "\n",
    "cur.execute(\"CREATE TABLE nodes_tags (id INTEGER, key TEXT, value TEXT, type TEXT, FOREIGN KEY (id) REFERENCES nodes(id));\") # use your column names here\n",
    "con.commit()\n",
    "\n",
    "with open('nodes_tags.csv','rb') as f: \n",
    "\n",
    "    data = csv.DictReader(f) \n",
    "    to_db = [(i['id'].decode(\"utf-8\"),i['key'].decode(\"utf-8\"),i['value'].decode(\"utf-8\"),i['type'].decode(\"utf-8\")) for i in data]\n",
    "    \n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes_tags (id,key,value,type) VALUES (?,?,?,?);\", to_db,)\n",
    "con.commit()\n",
    "\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS ways_tags; ''')\n",
    "con.commit()\n",
    "\n",
    "cur.execute(\"CREATE TABLE ways_tags (id INTEGER NOT NULL,key TEXT NOT NULL,value TEXT NOT NULL,type TEXT,FOREIGN KEY (id) REFERENCES ways(id));\") # use your column names here\n",
    "con.commit()\n",
    "\n",
    "with open('ways_tags.csv','rb') as f: \n",
    "    data = csv.DictReader(f)\n",
    "    to_db = [(i['id'].decode(\"utf-8\"),i['key'].decode(\"utf-8\"),i['value'].decode(\"utf-8\"),i['type'].decode(\"utf-8\")) for i in data]\n",
    "    \n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_tags (id,key,value,type) VALUES (?,?,?,?);\", to_db,)\n",
    "con.commit()\n",
    "\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS ways_nodes; ''')\n",
    "con.commit()\n",
    "\n",
    "cur.execute(\"CREATE TABLE ways_nodes (id INTEGER NOT NULL,node_id INTEGER NOT NULL, position INTEGER NOT NULL,FOREIGN KEY (id) REFERENCES ways(id),FOREIGN KEY (node_id) REFERENCES nodes(id));\") \n",
    "con.commit()\n",
    "\n",
    "with open('ways_nodes.csv','rb') as f: \n",
    "    data = csv.DictReader(f)\n",
    "    to_db = [(i['id'].decode(\"utf-8\"),i['node_id'].decode(\"utf-8\"),i['position'].decode(\"utf-8\")) for i in data]\n",
    "    \n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_nodes (id,node_id, position) VALUES (?,?,?);\", to_db,)\n",
    "con.commit()\n",
    "con.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
